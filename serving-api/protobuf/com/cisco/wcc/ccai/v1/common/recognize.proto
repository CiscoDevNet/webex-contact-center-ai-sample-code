/*
 This proto file has all the messages required for
 STT Service and its recognize gRPC calls
*/
syntax = "proto3";

package com.cisco.wcc.ccai.v1;



/*
 A streaming speech recognition result corresponding to a portion of the audio
 that is currently being processed.
*/
message StreamingRecognitionResult {
  /*
    Output only. May contain one or more recognition hypotheses (up to the
    maximum specified in `max_alternatives`).
    These alternatives are ordered in terms of accuracy, with the top (first)
    alternative being the most probable, as ranked by the recognizer.
  */
  repeated SpeechRecognitionAlternative alternatives = 1;

  /* Output only. If `false`, this `StreamingRecognitionResult` represents an
    interim result that may change. If `true`, this is the final time the
    speech service will return this particular `StreamingRecognitionResult`,
    the recognizer will not return any further hypotheses for this portion of
    the transcript and corresponding audio.
  */
  bool is_final = 2;

  /*
    Output only. Time offset of the end of this result relative to the
    beginning of the audio.
  */
  Duration result_end_time = 4;

  /*
    For multi-channel audio, this is the channel number corresponding to the
    recognized result for the audio from that channel.
    For audio_channel_count = N, its output values can range from '1' to 'N'.
  */
  int32 channel_tag = 5;

  /*
    Output only. The
    [BCP-47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt) language tag of the
    language in this result. This language code was detected to have the most
    likelihood of being spoken in the audio.
  */
  string language_code = 6;

  /*
   Whether or not recording offsets have been applied to the word
   alignment values. Otherwise the word alignment start and end times
   are only relative within the utterance.
  */
  bool has_applied_recording_offsets = 7;

  /*
    Zero or more integers representing the speaker ID of this
    result. This is usually derived from the speaker integers
    that are passed in the streaming request.
  */
  repeated uint32 speaker_ids = 8;


  /*
    The unix time in milliseconds which was received from the client
    for the StreamingRecognizeRequest that was last used to complete
    this utterance.
  */
  int64 last_packet_metrics_unix_timestamp_ms = 9;

  string message_type = 10; //message type

  // Event Based on user utterances.
  OutputEvent response_event = 11;

  enum Role{
    UNDEFINED=0; // Role - Undefined
    CALLER=1; // Role - Caller
    AGENT=2; // Role - Agent
  }

  Role role = 12;
}

/*
 Represents the Alternative hypotheses (a.k.a. n-best list).
*/
message SpeechRecognitionAlternative {
  // Output only. Transcript text representing the words that the user spoke.
  string transcript = 1;

  /*
    Output only. The confidence estimate between 0.0 and 1.0. A higher number
    indicates an estimated greater likelihood that the recognized words are
    correct. This field is set only for the top alternative of a non-streaming
    result or, of a streaming result where `is_final=true`.
    Not yet supported.
  */
  float confidence = 2;

  /*
    Output only. A list of word-specific information for each recognized word.
    Note: When `enable_speaker_diarization` is true, you will see all the words
    from the beginning of the audio.
  */
  repeated WordInfo words = 3;
}

/*
 Represents the Word-specific information for recognized words.
*/
message WordInfo {
  /*
   Output only. Time offset relative to the beginning of the audio,
   and corresponding to the start of the spoken word.
   This field is only set if `enable_word_time_offsets=true` and only
   in the top hypothesis.
   This is an experimental feature and the accuracy of the time offset can
   vary.
  */
  Duration start_time = 1;

  /*
   Output only. Time offset relative to the beginning of the audio,
   and corresponding to the end of the spoken word.
   This field is only set if `enable_word_time_offsets=true` and only
   in the top hypothesis.
   This is an experimental feature and the accuracy of the time offset can
   vary.
  */
  Duration end_time = 2;

  // Output only. The word corresponding to this set of information.
  string word = 3;
}

/*
 Represents the Duration object denoting seconds and nanos
*/
message Duration {
  /*
    Signed seconds of the span of time. Must be from -315,576,000,000
    to +315,576,000,000 inclusive. Note: these bounds are computed from:
    60 sec/min * 60 min/hr * 24 hr/day * 365.25 days/year * 10000 years
  */
  int64 seconds = 1;

  /*
    Signed fractions of a second at nanosecond resolution of the span
    of time. Durations less than one second are represented with a 0
    `seconds` field and a positive or negative `nanos` field. For durations
    of one second or more, a non-zero value for the `nanos` field must be
    of the same sign as the `seconds` field. Must be from -999,999,999
    to +999,999,999 inclusive.
  */
  int32 nanos = 2;
}

/*
 Represents the Status object with status code, message and description
*/
message Status {
  /*
    The status code, which should be an enum value of
    [google.rpc.Code][google.rpc.Code].
  */
  int32 code = 1;

  /*
    A developer-facing error message, which should be in English. Any
    user-facing error message should be localized and sent in the
    [google.rpc.Status.details][google.rpc.Status.details] field, or localized
    by the client.
  */
  string message = 2;

  // Longer description if available.
  string details = 3;
}

/*
 Return the Event based on the use input.
 Similar events will be returned for Voice / DTMF based inputs.
*/
enum OutputEvent {
  EVENT_UNSPECIFIED = 0;
  /*
    Triggers when user utter the first utterance in Voice Input mode or First DTMF is pressed in DTMF Input mode.
    This event to be used to BargeIn the prompt based on prompt barge-in flag.
    The event will be sent only if the current prompt being played is bargein enabled or prompt playing is complete.
  */
  EVENT_START_OF_INPUT = 1;
  // Sent when user utterance Voice / DTMF is complete.
  EVENT_END_OF_INPUT = 2;
  // Sent when utterance did not match any of the accepted input
  EVENT_NO_MATCH = 3;
  // Sent when no audio received with in the expected timeframe
  EVENT_NO_INPUT = 4;
}

/*
 Represents the RecognitionFlags object with boolean to enable or disable timers
*/
message RecognitionFlags {
  bool stall_timers = 1; // Whether to disable recognition timers. By default, timers start when recognition begins.
}

/*
 Represents the TimerInfo object with different stats
*/
message SpeechTimers {
  int32 no_input_timeout_ms = 1; // Maximum silence, in ms, allowed while waiting for user input after recognition timers are started. Default is 7000 ms. A value of -1 means no timeout.
  int32 complete_timeout_ms = 2; // Specify the duration of silence, in ms, after a valid recognition has occurred that determines the caller has finished speaking. Default is 0 (timer disabled).
  int32 incomplete_timeout_ms = 3; // Specify the duration of silence, in ms, after an utterance before concluding that the caller has finished speaking. Default is 1500 ms. A value of 0 disables the timer.
  int32 max_speech_timeout_ms = 4; // Maximum duration, in ms, of an utterance collected from the user. Default is 22000 ms (22 seconds). A value of -1 means no timeout.
}

/*
  DTMF Input Configurations
*/
message DTMFConfig {
  int32   inter_digit_timeout = 1;    // Timeout between two digits
  string  termchar = 2;               // Char to terminate the DTMF inputs
  int32   dtmf_length = 3;            // Max Length of DTMF to be collected
}

/*
 Represents the RecognitionResource object with attributes like grammar, language and weight
*/
message RecognitionResource {
  oneof grammar {
    BuiltInGrammar builtInGrammar = 1;                    // Name of a builtin resource supported by the installed language pack.
    InlineGrammar inlineGrammar = 5;
  }
  string language = 2;                              // Mandatory. Language and country (locale) code as xx-XX (2-letters format), e.g. en-US.
  int32 weight = 3;                               // Specifies the grammar's weight relative to other grammars active for that recognition. This value can range from 1 to 32767. Default is 1.
  string grammar_id = 4;                            // Specifies the id that Nuance Recognizer will use to identify the grammar in the recognition result. If not set, Nuance Recognizer will generate a unique one.
}

//BuildInGrammar enum
enum BuiltInGrammar {
  Boolean=0;
  Currency=1;
  Date=2;
  Digits=3;
  Number=4;
  Phone=5;
  Time=6;
  Alphanum=7;
  Amount=8;
  Ordinal_number=9;
  Cardinal_number=10;
}

/*
 InlineGrammar object with media type and grammar information
*/
message InlineGrammar {
  EnumMediaType media_type = 1;                     // The type of media used for the inline grammar data. If not specified, let Nuance Recognizer detect the media type.
  string grammar = 2;                                // Grammar data
}

//Media type enum
enum EnumMediaType {
  AUTOMATIC = 0;                                    // attempt to figure out the media type automatically.
  APPLICATION_SRGS_XML = 1;                         // "application/srgs+xml"
  APPLICATION_X_SWI_GRAMMAR = 2;                    // "application/x-swi-grammar"
  APPLICATION_X_SWI_PARAMETER= 3;                   // "application/x-swi-parameter"
}

message Model {
  // Name of the model to be used for recognition
  string model_name=1;
  // Variant of the model to be used for recognition
  string model_variant=2;
}


/*
 Represents a recognition request config
*/
message RecognitionRequestConfig {
  /*
   The encoding of the audio data sent in the request.

   All encodings support only 1 channel (mono) audio.
  */
  enum AudioEncoding {
    // Not specified.
    ENCODING_UNSPECIFIED = 0;

    // Uncompressed 16-bit signed little-endian samples (Linear PCM).
    LINEAR16 = 1;

    // 8-bit samples that compand 14-bit audio samples using G.711 PCMU/mu-law.
    MULAW = 3;

    // 8-bit samples that compand 14-bit audio samples using G.711 PCMU/a-law.
    ALAW = 4;
  }

  /*
    Encoding of audio data sent in all `RecognitionAudio` messages.
    This field is optional for `FLAC` and `WAV` audio files and required
    for all other audio formats. For details, see
    [AudioEncoding][google.cloud.speech.v1.RecognitionConfig.AudioEncoding].
  */
  AudioEncoding encoding = 1;

  /*
    Sample rate in Hertz of the audio data sent in all
    `RecognitionAudio` messages.
  */
  int32 sample_rate_hertz = 2;

  // Currently, only en-US is supported.
  string language_code = 3;

  /*
    Optional. Maximum number of recognition hypotheses to be returned.
    Specifically, the maximum number of `SpeechRecognitionAlternative` messages
    within each `SpeechRecognitionResult`.
    The server may return fewer than `max_alternatives`.
    Valid values are `0`-`30`. A value of `0` or `1` will return a maximum of
    one. If omitted, will return a maximum of one.
   */
  int32 max_alternatives = 4;

  /*
    Optional. If set to `true`, the server will attempt to filter out
    profanities, replacing all but the initial character in each filtered word
    with asterisks, e.g. "f***". If set to `false` or omitted, profanities
    won't be filtered out.
    Not yet supported.
  */
  bool profanity_filter = 5;

  /*
    Optional. array of [SpeechContext][google.cloud.speech.v1.SpeechContext].
    A means to provide context to assist the speech recognition. For more
    information, see [Phrase Hints](/speech-to-text/docs/basics#phrase-hints).
  */
  repeated SpeechContext speech_contexts = 6;

  /*
    Optional. If `false` or omitted, the recognizer will perform continuous
    recognition (continuing to wait for and process audio even if the user
    pauses speaking) until the client closes the input stream (gRPC API) or
    until the maximum time limit has been reached. May return multiple
    `StreamingRecognitionResult`s with the `is_final` flag set to `true`.

    If `true`, the recognizer will detect a single spoken utterance. When it
    detects that the user has paused or stopped speaking, it will return an
    `END_OF_SINGLE_UTTERANCE` event and cease recognition. It will return no
    more than one `StreamingRecognitionResult` with the `is_final` flag set to
    `true`.
  */
  bool single_utterance = 7;

  /*
    Optional If `true`, interim results (tentative hypotheses) may be
    returned as they become available (these interim results are indicated with
    the `is_final=false` flag).
    If `false` or omitted, only `is_final=true` result(s) are returned.
  */
  bool interim_results = 8;

  /*
    Optional. If 'true', adds punctuation to recognition result hypotheses.
    Not yet supported.
  */
  bool enable_automatic_punctuation = 9;

  // Optional. Pass timer info for the recognizer
  SpeechTimers speech_timers  = 10;

  // A balance between detecting speech and noise (breathing, etc.), 0 to 1.0. 0 means ignore all noise, 1.0 means interpret all noise as speech. Default is 0.5.
  float speech_detection_sensitivity = 11;

  // Maximum number of n-best hypotheses to return. Range is 0 to 999. Additional CPU cycles needed if > 5. Default is 2.
  int32 nbest = 12;

  // When the score of the first n-best entry is less than the value of confidencelevel, the recognition will return a no-match. Range is 0 to 1.0. Default is 0 (all utterances accepted).
  float confidence_level = 13;

  //Channel number which has the caller stream in a multichannel file URL. Other streams will be considered as Agent Stream.
  //Default value will be based on RTMS usecase
  int32 caller_channel_number = 14;

  // Model to be used for recognition
  Model model = 15;
}

/*
 Represents a speech context message
*/
message SpeechContext {
  /*
   Optional. A list of strings containing words and phrases "hints" so that
   the speech recognition is more likely to recognize them. This can be used
   to improve the accuracy for specific words and phrases, for example, if
   specific commands are typically spoken by the user. This can also be used
   to add additional words to the vocabulary of the recognizer.
  */
  repeated string phrases = 1;
}
